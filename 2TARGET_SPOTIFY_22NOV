---
title: "SPOTIFY_2TARGET"
output: html_document
date: "2023-09-29"
---


codigo modificado, hemos puesto 2 target (0 hasta 85 y 1 en adelante), obteniendo en KNN 5 falsos positivos y 1 acierto. 22 de noviembre

# 1. Business Understanding

Este modelo se va a centrar en realizar un análisis de las canciones que han sido hit entre los años 2000-2022 en la plataforma de Spotify, con el objetivo de determinar qué canción será un hit de máxima popularidad.

Preguntas que nos hacemos:

#### Super hits?
#### Que artistas tienen mas popularidad, actuales o antiguos?
#### Canciones largas son superhists?
#### El instrumental o el nivel de sonido es importante?
#### Canciones con un nivel de sonido bajo es posible que sea un superhit?
#### Una cancion con poca danzabilidad puede ser superhit?


# 2. Data Understanding

## 2.1. Descripción de los datos obtenidos de Spotify (Obtenidos via Kaggle)

***Variables cualitativas*** 
  
  * playlist_url: dirección de la playlist que contiene los hits de cada año
  
  * track_id: id de la canción en Spotify
  
  * track_name: nombre de la canción
  
  * album: álbum al que pertenece la canción

  * artist_id: id del artista que realiza la canción

  * artist_genres: géneros musicales a los que está asociado el artista que realiza la canción
  
***Variables cuantitativas***

  * year: año de la canción

  * track_popularity: popularidad de la canción en una escala de 0-100

  * artist_popularity: popularidad del artista de la canción en una escala de 0-100

  * danceability: capacidad de una canción de ser bailable en escala de 0-1

  * energy: medida en escala de 0-1 que representa una medida perceptual de la intensidad y actividad de la canción

  * key: clave musical en la que se encuentra la canción, en notación estándar de las notas. Por ejemplo 0 = C, 1 = C♯/D♭, 2 = D

  * loudness: volumen general de la canción medido en decibelios (dB)

  * mode: modalidad (mayor o menor) de la canción. La modalidad "mayor" está representada por 1 y la modalidad "menor" por 0

  * speechiness: presencia de palabras habladas en la canción respecto a la duración de esta, en escala de 0-1

  * acousticness: medida de confianza en escala de 0-1 de la acústica de la canción

  * instrumentalness: presencia de instrumentos en la canción respecto a la duración de esta, en escala de 0-1

  * liveness: medida de la identificación de sonidos o señales que indican la existencia de un público o audiencia en la grabación musical de la canción

  * valence: medida en escala de 0-1 que describe la positividad musical que transmite la canción

  * tempo: velocidad y ritmo estimado de la canción en pulsaciones por minuto (BPM)

  * duration_ms: duración de la canción en milisegundos

  * time_signature: compás o ritmo estimado de la canción


## 2.2. Paquetes importados

```{r paquetes}

library(dplyr)
library(ggplot2)
library(MASS)
library(graphics)
library(ellipse)   # Correlaciones
library(class)
library(rpart) # para árboles de decisión

```


## 2.3. Importación de datos

```{r cargamos los datos}

#str(datosSPOTIFY)

# Seleccion datos David:
#"C:\\Users\\David\\Desktop\\playlist_2010to2022.csv"

# Ruben:
# "C:\\Users\\ruben_zbu59h5\\OneDrive\\Escritorio\\4º Matematicas\\Primer cuatri\\Mineria de Datos\\Trabajo\\MDAT\\playlist_2010to2022.csv"

# Isma:
#C:\\UNIVERSIDAD\\4º\\Minería de datos\\MDAT\\datos\\playlist_2010to2022.csv


datosBase <- read.csv("C:\\Users\\David\\Desktop\\MDAT\\playlist_2010to2022.csv", sep=",", dec=".")
```


## 2.4. Limpiar datos

Veamos cuantos NA tenemos, para buscar solución e interpretación de estos.

```{r problema NA}
sum(is.na(datosBase))
which(is.na(datosBase)) #sacamos los NA
#tenemos que eliminar la fila 448 (cancion: These Words , Unwritten)
datosModificados = datosBase[-c(448), ]
datosModificados = datosModificados[,-1] # Eliminamos la primera columna de los datos (url playlist)
```

Como solo teníamos una canción con sus variables NA, en cómputo queneral es algo que no está aportando nada ya que simplemente es un error o falta de información en la base de datos. Al ser mínimo, podemos poner la media de cada variable o simplemente quitarlo ya que no es significativo. Hemos optado por quitarlo.

Quitamos la variable que incluye la url de la playlist porque no aporta nada diferente a la variable que incluye los años en los que fueron hit las canciones, porque cada playlist es el top 100 canciones de un año.

Veamos un pequeño resumen de nuestros datos (con un ejemplo de visualización):

```{r}

dim(datosModificados) # vemos las dimensiones de nuestros datos (numero de registros y variables)
summary(datosModificados) # resumen de los datos por cada variable
head(datosModificados) # Imprimimos algunos datos como ejemplo
```



Las variables playlist_url, track_id y artist_id son candidatas a que las limpiemos.

```{r quitar ids?}

```



PREGUNTAR.......................................................
```{r quitar ids problema nombres por ser diferente año}

frecuencia_nombres <- table(datosModificados$track_name)

mean(frecuencia_nombres)

nombres_frecuencia_mayor_1 <- names(frecuencia_nombres[frecuencia_nombres > 1])

# Muestra los nombres con frecuencia mayor que 1
print(nombres_frecuencia_mayor_1)



```


Salen bastantes nombres repetidos, hemos visto que es porque aparecen varios años. Haciendo una investigación sobre la fuente, se debe a que estos hits se extraen de playlists creadas por Spotify de cada año. Por esa razón, si una canción aparece en la playlist de top hits 2021 y a la vez en la de 2022, la canción aparece 2 veces en la base de datos, solo con el año cambiado.



## 2.5. Definición de la variable Target


```{r cuantiles}

quantile(datosModificados$track_popularity, 0.05)
quantile(datosModificados$track_popularity, 0.5)
quantile(datosModificados$track_popularity, 0.95)
```

Vamos a dividir nuestra variable objetivo (target) en 4 dependiendo de los cuantiles; 

- Si trackpopularity es menor que 54, se establece en 0. -> Representa el 5% de los datos MENOS POPULARES
- Si trackpopularity está entre 54 y 75, se establece en 1.  -> Representa el 45% de los datos 
- Si trackpopularity está entre 75 y 85, se establece en 2. -> Representa el 45% de los datos 
- Si trackpopularity está entre 86 y 100, se establece en 3.-> Representa el 5% de los datos MAS POPULARES


```{r variable target}

#quantile(datosModificados$track_popularity)

datosModificados = datosModificados %>% mutate(target = if_else(track_popularity <= 54, 0, ifelse(54 < track_popularity  & track_popularity <= 72, 0, ifelse(track_popularity > 72 & track_popularity < 85, 0, 1 )))) #quantile(datosmodificados$track_popularity, 0.05)

datosModificados = datosModificados[,-c(4)]
  
```



## 2.6. Filtrar géneros

Debido a que la variable artist_genres nos da varios géneros en una misma variable cualitativa, vamos a quedarnos con el género más significativo para cada caso.

Esto sería un trabajo a realizar junto a un experto en el tema, pero como no contamos con uno, lo que hemos hecho ha sido ver todos los géneros que aparecen y, tras previa búsqueda de información sobre cada género, hemos agrupado cada subgénero en su género principal.

Además, hemos creado un género llamado 'Otros', donde se guardan los que NO pertenecen a alguno de los principales.

Como dato a tener en cuenta, la mayoría de subgeneros se incluirían en Pop, ya que al ser una base de datos de hits, la propia definición de este género nos obligaría a meterlos, resultando en una pérdida importante de información. 

Para solucionar esto, si cualquiera de los otros géneros aparece en esa observación, nos quedamos con el otro, en vez de coger siempre Pop. Todo esto es posible gracias a que estamos haciendo esta construcción mediante else-ifs.


```{r filtro de generos}
for (i in datosModificados$artist_genres) {
  if( grepl("rap",i) ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'rap'
  }
  else if( grepl("r&b",i) ||  grepl("soul",i) ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'r&b'
  }
    else if( grepl("urban contemporary",i) ||  grepl("hop",i) || grepl("urbano latino",i) || grepl("reggaeton",i)  ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'urban'
    }
    else if( grepl("metal",i) ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'metal'
      }
  else if( grepl("rock",i) ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'rock'
      }
  else if( grepl("edm",i) ||  grepl("trance",i) ||  grepl("electronica",i) ||  grepl("dance",i) ||  grepl("house",i)  || grepl("uk garage",i)){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'edm'
  }
  else if( grepl("k pop",i) ){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'k pop'
  }
      else if ( grepl("country",i)){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'country'
  }
  else if ( grepl("pop",i) || grepl("boy band",i) || grepl("mellow",i)){
    datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'pop'
  }
  else ( datosModificados$artist_genres[datosModificados$artist_genres == i] <- 'otros')

}

datosModificados$artist_genres <- as.factor(datosModificados$artist_genres)

```


Veamos cómo quedan repartidos los géneros, para verificar que la separación realizada tiene algún sentido.

```{r graficos generos}


barplot(table(datosModificados$artist_genres))
table(datosModificados$artist_genres)


```




## 2.7. Clasificación en Train, Test y Validación


Dividimos los datos en diferentes tablas que utilizaremos para entrenar, hacer pruebas y validar resultado.

```{r}

n_total= dim(datosModificados)[1]
n_train = n_total * .5     # La mitad de los datos son para train
n_test = n_total *.25      # 1/4 para probar

indices_totales = seq(1:n_total)
indices_train = sample(indices_totales, n_train)
indices_test = sample(indices_totales[-indices_train],n_test)

datos = datosModificados[indices_train,]    # Los datos de train, como son los más utilizados los llamamos datos
datos_test = datosModificados[indices_test,]
datos_validation = datosModificados[-c(indices_test,indices_train),]
```






# 3. Analisis Exploratorio

Para comenzar con nuestro anánisis, lo primero que vamos a hacer es crear un Data Frame solo con las variables cuantitativas. Y por otro lado, uno con las variables continuas.

```{r creacion dataframe de cuantitativos y continuos para los datos train}

datosCuant = cbind( year = datos$year, artist_popularity = datos$artist_popularity ,danceability = datos$danceability,energy = datos$energy,loudness = datos$loudness, speechiness = datos$speechiness,acousticness =  datos$acousticness, instrumentalness = datos$instrumentalness, liveness = datos$liveness,valence = datos$valence,tempo= datos$tempo,duration_ms = datos$duration_ms)

datosCont = cbind( danceability = datos$danceability,energy = datos$energy,loudness = datos$loudness, speechiness = datos$speechiness,acousticness =  datos$acousticness, instrumentalness = datos$instrumentalness, liveness = datos$liveness,valence = datos$valence,tempo= datos$tempo,duration_ms = datos$duration_ms)

```


```{r creacion dataframe de cuantitativos y continuos para los datos test}

datosCuant_Test = cbind( year = datos_test$year, artist_popularity = datos_test$artist_popularity ,danceability = datos_test$danceability,energy = datos_test$energy,loudness = datos_test$loudness, speechiness = datos_test$speechiness,acousticness =  datos_test$acousticness, instrumentalness = datos_test$instrumentalness, liveness = datos_test$liveness,valence = datos_test$valence,tempo= datos_test$tempo,duration_ms = datos_test$duration_ms)

datosCont_Test = cbind( danceability = datos_test$danceability,energy = datos_test$energy,loudness = datos_test$loudness, speechiness = datos_test$speechiness,acousticness =  datos_test$acousticness, instrumentalness = datos_test$instrumentalness, liveness = datos_test$liveness,valence = datos_test$valence,tempo= datos_test$tempo,duration_ms = datos_test$duration_ms)

```



Veamos una comparación general de todas nuestras variables cuantivas:

```{r pair cuant}

pairs(datosCuant)

```

Ahora, una comparación general de todas nuestras variables CONTINUAS:

```{r pair cont}

pairs(datosCont)

```


Veamos la correlación existente entre las variables cuantitativas:

```{r}
plotcorr(cor(datosCuant)) # cuanto mas se asemejen las figuras a una elipse mayor es la correlacion
```

En las variables que no son continuas (year y popularities) no podemos apreciar mucha correlación con las demás, por ello, veamos solo las continuas:

```{r}
plotcorr(cor(datosCont)) # cuanto mas se asemejen las figuras a una elipse mayor es la correlacion
```

Veamos un gráfico de densidades de las variables: 

```{r}
#ggplot(datosCuant, aes(x= lprice, fill=color)) + geom_density(alpha=0.3)
```



```{r}
plot(datos$energy , datos$loudness)
plot(datos$tempo , datos$danceability)


plot( datos$target, datos$loudness)
plot(datos$target, datos$danceability)
plot(datos$target, datos$energy)
plot(datos$target, datos$instrumentalness)

```



# 4. Estudio de PCA

Vamos a hacer el estudio de PCA de las variables continuas, primero lo haremos con éstas escaladas y despues sin escalar, para ver si la diferencia nos infiere alguna conclusión.

# Variables continuas ESCALADAS

Primero escalamos los datos continuos

```{r Creacion de datosContEscalados}

datosContEscalados = scale(datosCont)
n = dim(datosContEscalados)[1]
p = dim(datosContEscalados)[2]
```

Una vez tenemos los datos escalados, hacemos un análisis de las componentes principales para éstos.


```{r Analisis PCA para datos escalados}
analisisPCAEscalados <- prcomp(datosContEscalados, scale= TRUE)

cov(datosContEscalados)

analisisPCAEscalados

```

Veamos la información acumulada que nos va aportando cada componente principal

```{r}
summary(analisisPCAEscalados)

#hacer varias componentes principales incluyendo popularidad o no
```


Podemos considerar que la proporción acumulada a partir de PC4 (0.6) no aumenta de manera considerable y es un porcentaje suficientemente grande de explicabilidad de los datos.



```{r Elección de numero de clusters para escalados}
#clusters2.datosCont = kmeans(datosContEscalados, centers=2, nstart=25)

#Inicializamos el vector
SSW <- vector(mode = "numeric", length = 15)


#Variabilidad de todos los datos, es decir, todos los datos como un único cluster
SSW[1] <- (n - 1) * sum(apply(datosContEscalados,2,var)) 

#Variabilidad de cada modelo, desde 2 clusters hasta 15 clusters
for (i in 2:15) SSW[i] <- sum(kmeans(datosContEscalados,centers=i,nstart=25)$withinss)



#Dibujamos un gráfico con el resultado
plot(1:15, SSW, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col="steelblue4")

```

Se aprecia que a partir de 5 o 6 hay un "codo" por lo que nos vamos a quedar con 5 clusters.


# Variables continuas NO ESCALADAS

Veamos que si no hubiesemos escalado los datos, los resultados cambiarian, pero únicamente los de la elección del número de clusters, el resto de datos (PCA) son idénticos.

```{r }
nCont = dim(datosCont)[1]
pCont = dim(datosCont)[2]
```

Análisis de las componentes principales para los datos sin escalar.


```{r}
analisisPCA_NoEscalados <- prcomp(datosCont, scale= TRUE)

cov(datosCont)

analisisPCA_NoEscalados

```

Veamos la información acumulada que nos va aportando cada componente principal

```{r}
summary(analisisPCA_NoEscalados)

#hacer varias componentes principales incluyendo popularidad o no
```


Podemos considerar que la proporción acumulada a partir de PC4 (0.6) no aumenta de manera considerable y es un porcentaje suficientemente grande de explicabilidad de los datos.



```{r Elección de numero de clusters para no escalados}
#Inicializamos el vector
SSW2 <- vector(mode = "numeric", length = 15)

#Variabilidad de todos los datos, es decir, todos los datos como un único cluster
SSW2[1] <- (n - 1) * sum(apply(datosCont,2,var)) 

#Variabilidad de cada modelo, desde 2 clusters hasta 15 clusters
for (i in 2:15) SSW2[i] <- sum(kmeans(datosCont,centers=i,nstart=25)$withinss)

#Dibujamos un gráfico con el resultado
plot(1:15, SSW2, type="b", xlab="Number of Clusters", ylab="Sum of squares within groups",pch=19, col="steelblue4")

```

Se aprecia que a partir de 3 o 4 hay un "codo" por lo que nos vamos a quedar con 5 clusters.


# 5. Técnicas de aprendizaje supervisado

## 5.1. Cluster

### Volvemos a los datos escalados para calcular los cluster


Calculamos los 5 cluster, para 25 casos y se queda con el mejor
                   
```{r}
# k-means para 5 grupos y 25 arranques diferentes

clusters5.datosContEscalados <- kmeans(datosContEscalados, 5, nstart = 25)

clusters5.datosContEscalados

```

Podemos ver en la ejecución anterior a que cluster pertenece cada observación.

Ahora veamos cuales son los centroides

```{r}
centroides = aggregate(datosContEscalados,by=list(clusters5.datosContEscalados$cluster),FUN=mean)

t(centroides)
```

Por ejemplo, para el cluster 1 y la variable danceability, el centroide es 0.20288349


Ahora representamos los cluster

```{r}
# Dibujamos los clusters en el scatterplot (variables 2a2)

nk=5 # nk es el numero de clusters 
pairs(datosContEscalados, col= clusters5.datosContEscalados$cluster,pch=19)

points(clusters5.datosContEscalados$centers, col = 1:nk, pch = 19, cex=2)

```


Ahora lo hacemos con 6 clusters

```{r}
# k-means para 6 grupos y 25 arranques diferentes

clusters6.datosContEscalados <- kmeans(datosContEscalados, 6, nstart = 25)

clusters6.datosContEscalados

centroides = aggregate(datosContEscalados,by=list(clusters6.datosContEscalados$cluster),FUN=mean)

t(centroides)

# Dibujamos los clusters en el scatterplot (variables 2a2)

nk=6 # nk es el numero de clusters 
pairs(datosContEscalados, col= clusters6.datosContEscalados$cluster,pch=19)

points(clusters6.datosContEscalados$centers, col = 1:nk, pch = 19, cex=2)


```

Esto era representando los cluster con todos los datos.

Ahora repetimos el proceso pero con las PCA.



```{r}
# Guardamos el vector con el cluster correspondiente a cada país
clusters5.datosContEscalados_PCA <- clusters5.datosContEscalados$cluster
# Vamos a hacer PCA para poder graficar los clusters en 2dimensiones!
library(cluster)

clusplot(datosContEscalados, clusters5.datosContEscalados_PCA, color=TRUE, shade=TRUE, labels=6,lines=0)
```




### Volvemos a los datos NO escalados para calcular los cluster


Calculamos los 3 cluster, para 25 casos y se queda con el mejor
                   
```{r}
# k-means para 5 grupos y 25 arranques diferentes

clusters3.datosCont <- kmeans(datosCont, 5, nstart = 25)

clusters3.datosCont

```

Podemos ver en la ejecución anterior a que cluster pertenece cada observación.

Ahora veamos cuales son los centroides

```{r}
centroides = aggregate(datosCont,by=list(clusters3.datosCont$cluster),FUN=mean)

t(centroides)
```

Por ejemplo, para el cluster 1 y la variable danceability, el centroide es 0.20288349


Ahora representamos los cluster

```{r}
# Dibujamos los clusters en el scatterplot (variables 2a2)

nk=3 # nk es el numero de clusters 
pairs(datosCont, col= clusters3.datosCont$cluster,pch=19)

points(clusters3.datosCont$centers, col = 1:nk, pch = 19, cex=2)

```


Ahora lo hacemos con 4 clusters

```{r}
# k-means para 4 grupos y 25 arranques diferentes

clusters4.datosCont <- kmeans(datosCont, 4, nstart = 25)

clusters4.datosCont

centroides = aggregate(datosCont,by=list(clusters4.datosCont$cluster),FUN=mean)

t(centroides)

# Dibujamos los clusters en el scatterplot (variables 2a2)

nk=4 # nk es el numero de clusters 
pairs(datosCont, col= clusters4.datosCont$cluster,pch=19)

points(clusters4.datosCont$centers, col = 1:nk, pch = 19, cex=2)


```

Esto era representando los cluster con todos los datos.

Ahora repetimos el proceso pero con las PCA.

```{r}
# Guardamos el vector con el cluster correspondiente a cada país
clusters4.datosCont_PCA <- clusters4.datosCont$cluster
# Vamos a hacer PCA para poder graficar los clusters en 2dimensiones!
library(cluster)

clusplot(datosCont, clusters4.datosCont_PCA, color=TRUE, shade=TRUE, labels=6,lines=0)
```



```{r PRUEBA PCA CLUSTER 2 a 2 DATOS ESCALADOS}

clusters5.datosContEscalados_PCA <- clusters5.datosContEscalados$cluster
# Seleccionamos las 4 primeras comp principales
pca_escogidos <- analisisPCAEscalados$x[,1:4]
# Agregamos la información del cluster
pca_escogidos <- cbind(pca_escogidos, Cluster = clusters5.datosContEscalados_PCA)
# Creamos los graficos de dispersión por pares
pairs(pca_escogidos, col = clusters5.datosContEscalados_PCA, pch = 16)


```

Podemos observar que hay un cluster que está separado del resto.


```{r PRUEBA PCA CLUSTER 2 a 2 DATOS SIN ESCALAR}
clusters4.datosCont_PCA <- clusters4.datosCont$cluster
pca_escogidos <- analisisPCA_NoEscalados$x[,1:4]
pca_escogidos <- cbind(pca_escogidos, Cluster = clusters4.datosCont_PCA)
pairs(pca_escogidos, col = clusters4.datosCont_PCA, pch = 16)

```

Para los datos no escalados los cluster no están muy definidos, a excepción de uno de los cluster que está diferenciado del resto.



## 5.2. KNN

```{r KNN}
KNN <- factor(datos$target, labels = c("0", "1"))
n <- length(datosModificados$year)^((4)/(22+4)) # 23 es el numero de variables que tenemos
n = round(n)
modelo_KNN <- knn(train = datosCuant, test = datosCuant_Test, cl = KNN, k = n)

tablaKNN = table(modelo_KNN, datos_test$target)
sum(diag(tablaKNN))/sum(tablaKNN)

diag(tablaKNN)[1]/sum(tablaKNN)
diag(tablaKNN)[2]/sum(tablaKNN)
diag(tablaKNN)[3]/sum(tablaKNN)
diag(tablaKNN)[4]/sum(tablaKNN)
```


Hemos obtenido que hay poco acierto (menos del 50%), por lo que nuestro estudio con 4 clases en la variable target quizás es mejor reducirlo a 2 clases, puesto que además, en la clase 1 y la clase 4 obtenemos 0% de acierto.


## 5.3 Arbol de decisión
```{r Convertimos las variables cualitativas a factores, sólo de los datos train}
datos[,2] = as.factor(datos[,2])
datos[,3] = as.factor(datos[,3])
datos[,5] = as.factor(datos[,5])
datos[,6] = as.factor(datos[,6])
datos[,7] = as.factor(datos[,7])
set.seed(123) # semilla para estudiar siempre la misma combinación
```



```{r Árbol de decisión}
library(rpart)

# Seleccionar las primeras 20 filas para entrenar el modelo (ajusta según tus necesidades)
datos_train_SIN_IDs <- datos[,-c(1,2,3,4,6,5)] # y sin nombres de canciones, album, artistas...

# Ajustar el modelo de árbol de decisión
# Puedes cambiar "information" a "gini" si prefieres el índice de Gini

datos.rp_cp1 <- rpart(datos_train_SIN_IDs[,16] ~., data = datos_train_SIN_IDs[,1:15], method = "class", cp = 0.0001, parms = list(split = "information"))

# Visualizar la complejidad del árbol
plotcp(datos.rp_cp1)

# Mostrar información sobre la complejidad del árbol
printcp(datos.rp_cp1)
```


```{r Árbol de decisión por gini}
library(rpart)

# Ajustar el modelo de árbol de decisión
# Puedes cambiar "information" a "gini" si prefieres el índice de Gini

datos.rp_cp1 <- rpart(datos_train_SIN_IDs[,16] ~., data = datos_train_SIN_IDs[,1:15], method = "class", cp = 0.0001, parms = list(split = "gini"))

# Visualizar la complejidad del árbol
plotcp(datos.rp_cp1)

# Mostrar información sobre la complejidad del árbol
printcp(datos.rp_cp1)
```
